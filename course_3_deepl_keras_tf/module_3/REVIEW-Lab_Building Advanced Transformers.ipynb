{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m606.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 pyarrow-20.0.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m183.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.3.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.5 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.3.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:44:26.294582: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-06 10:44:26.295964: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-06 10:44:26.301378: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-06 10:44:26.314934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751798666.337256     299 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751798666.343719     299 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751798666.367953     299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751798666.367985     299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751798666.367987     299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751798666.367989     299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-06 10:44:26.374144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 10:52:47.620548: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - loss: 10.7337 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1933 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2172 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1844 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1550 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1940 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.1084 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1245 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.1182 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.1056 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1086 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1360 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0810 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1029 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0741 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0498 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0435 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0370 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0533 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0340 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x731c2034bda0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transformers_20_epoch_32_bs.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 326ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(0.5)(flatten) \n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "# Build the model \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 5.4329  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 1.0502 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.5263 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1892 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1090 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0726 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0538 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0494 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0424 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0424 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0428 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0341 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0292 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0292 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0354 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0229 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0202 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0227 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0208 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x731bddaec050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transformers_20_epoch_32_bs_w_dropout.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 316ms/step - loss: 0.0014\n",
      "Test loss with dropout: 0.0009576678858138621\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss with dropout: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The loss is pretty low after adding the dropout.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 625ms/step - loss: 0.0251 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 627ms/step - loss: 0.0281 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 627ms/step - loss: 0.0332 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 631ms/step - loss: 0.0343 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 626ms/step - loss: 0.0274 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 633ms/step - loss: 0.0306 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 637ms/step - loss: 0.0206 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 628ms/step - loss: 0.0360 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 0.0308 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 624ms/step - loss: 0.0165 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 623ms/step - loss: 0.0140 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 624ms/step - loss: 0.0130 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 627ms/step - loss: 0.0107 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 620ms/step - loss: 0.0121 \n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 617ms/step - loss: 0.0121 \n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 624ms/step - loss: 0.0151 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 625ms/step - loss: 0.0198 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 634ms/step - loss: 0.0338 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 628ms/step - loss: 0.0195 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 0.0111 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 311ms/step - loss: 0.0156\n",
      "Test loss with batch size 16: 0.01238571759313345\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transformers_20_epoch_16_bs_w_dropout.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 314ms/step - loss: 0.0156\n",
      "Test loss with batch size 16: 0.01238571759313345\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_16_bs = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss_16_bs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0107 \n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0044\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0028\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0025\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0023\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0029\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0031\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0028\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0033\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0032\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0021\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0023\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0025\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0023\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0021\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0020\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x731bdf69a150>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transformers_20_epoch_64_bs_w_dropout.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 314ms/step - loss: 0.0015\n",
      "Test loss with batch size 64: 0.0010530296713113785\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_64_bs = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss_64_bs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch size `batch_size = 32` has the lowest loss score followed by `batch_size=64` and lastly `batch_size=16` with the highest loss score. It is interesting here because there is some trade-off happening with batch size. But surely a small batch size does not help because it takes the longest. WHen the batch size is too high that doesn't help either.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 0.2854 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2955 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.3008 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2973 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2969 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2948 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2962 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2920 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2914 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.3026 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2969 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2848 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 1.2609 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 2.3895 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 2.3938 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 2.3699 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 2.3672 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 2.4044 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 2.3527 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 2.3870 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x731bdf630fb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transformers_20_epoch_32_bs_w_dropout_activ_tanh.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 312ms/step - loss: 1.7508\n",
      "Test loss with tanh activation: 2.3743340969085693\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_tanh = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss_tanh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oooh Looks pretty bad for the loss function when using `tanh` activation function. This makes sense, because the output is a continuous variable and `tanh` activation function is typically used in classification problems. Again important to keep in mind the data is synthetically generated, but more importantly we can set up an advanced transformer setup.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
